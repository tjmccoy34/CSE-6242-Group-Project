{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "660154c1-2526-4ae6-abee-be51ae836d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1999 ...\n",
      "1999 done.\n",
      "Downcasting floats.\n",
      "Processing 2000 ...\n",
      "2000 done.\n",
      "Downcasting floats.\n",
      "Processing 2001 ...\n",
      "2001 done.\n",
      "Downcasting floats.\n",
      "Processing 2002 ...\n",
      "2002 done.\n",
      "Downcasting floats.\n",
      "Processing 2003 ...\n",
      "2003 done.\n",
      "Downcasting floats.\n",
      "Processing 2004 ...\n",
      "2004 done.\n",
      "Downcasting floats.\n",
      "Processing 2005 ...\n",
      "2005 done.\n",
      "Downcasting floats.\n",
      "Processing 2006 ...\n",
      "2006 done.\n",
      "Downcasting floats.\n",
      "Processing 2007 ...\n",
      "2007 done.\n",
      "Downcasting floats.\n",
      "Processing 2008 ...\n",
      "2008 done.\n",
      "Downcasting floats.\n",
      "Processing 2009 ...\n",
      "2009 done.\n",
      "Downcasting floats.\n",
      "Processing 2010 ...\n",
      "2010 done.\n",
      "Downcasting floats.\n",
      "Processing 2011 ...\n",
      "2011 done.\n",
      "Downcasting floats.\n",
      "Processing 2012 ...\n",
      "2012 done.\n",
      "Downcasting floats.\n",
      "Processing 2013 ...\n",
      "2013 done.\n",
      "Downcasting floats.\n",
      "Processing 2014 ...\n",
      "2014 done.\n",
      "Downcasting floats.\n",
      "Processing 2015 ...\n",
      "2015 done.\n",
      "Downcasting floats.\n",
      "Processing 2016 ...\n",
      "2016 done.\n",
      "Downcasting floats.\n",
      "Processing 2017 ...\n",
      "2017 done.\n",
      "Downcasting floats.\n",
      "Processing 2018 ...\n",
      "2018 done.\n",
      "Downcasting floats.\n",
      "Processing 2019 ...\n",
      "2019 done.\n",
      "Downcasting floats.\n",
      "Processing 2020 ...\n",
      "2020 done.\n",
      "Downcasting floats.\n",
      "Processing 2021 ...\n",
      "2021 done.\n",
      "Downcasting floats.\n",
      "Processing 2022 ...\n",
      "2022 done.\n",
      "Downcasting floats.\n",
      "Processing 2023 ...\n",
      "2023 done.\n",
      "Downcasting floats.\n",
      "Processing 2024 ...\n",
      "2024 done.\n",
      "Downcasting floats.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macsa\\anaconda3\\Lib\\site-packages\\IPython\\core\\formatters.py:347: FutureWarning: Index.format is deprecated and will be removed in a future version. Convert using index.astype(str) or index.map(formatter) instead.\n",
      "  return method()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>fantasy_player_id</th>\n",
       "      <th>fantasy_player_name</th>\n",
       "      <th>fp</th>\n",
       "      <th>games</th>\n",
       "      <th>ppg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999</td>\n",
       "      <td>00-0008241</td>\n",
       "      <td>E.James</td>\n",
       "      <td>357.600002</td>\n",
       "      <td>15.0</td>\n",
       "      <td>23.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999</td>\n",
       "      <td>00-0005092</td>\n",
       "      <td>M.Faulk</td>\n",
       "      <td>354.700004</td>\n",
       "      <td>14.0</td>\n",
       "      <td>25.335715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999</td>\n",
       "      <td>00-0007024</td>\n",
       "      <td>M.Harrison</td>\n",
       "      <td>344.600005</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.973334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999</td>\n",
       "      <td>00-0005883</td>\n",
       "      <td>E.George</td>\n",
       "      <td>287.800003</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.186667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999</td>\n",
       "      <td>00-0015218</td>\n",
       "      <td>J.Smith</td>\n",
       "      <td>285.100003</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.006667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13979</th>\n",
       "      <td>2024</td>\n",
       "      <td>00-0039139</td>\n",
       "      <td>J.Gibbs</td>\n",
       "      <td>306.800002</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13980</th>\n",
       "      <td>2024</td>\n",
       "      <td>00-0032764</td>\n",
       "      <td>D.Henry</td>\n",
       "      <td>306.300003</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.143750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13981</th>\n",
       "      <td>2024</td>\n",
       "      <td>00-0039040</td>\n",
       "      <td>D.Achane</td>\n",
       "      <td>281.800002</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.612500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13982</th>\n",
       "      <td>2024</td>\n",
       "      <td>00-0035700</td>\n",
       "      <td>J.Jacobs</td>\n",
       "      <td>281.500002</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13983</th>\n",
       "      <td>2024</td>\n",
       "      <td>00-0037840</td>\n",
       "      <td>K.Williams</td>\n",
       "      <td>272.100003</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.006250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       season fantasy_player_id fantasy_player_name          fp  games   \n",
       "0        1999        00-0008241             E.James  357.600002   15.0  \\\n",
       "1        1999        00-0005092             M.Faulk  354.700004   14.0   \n",
       "2        1999        00-0007024          M.Harrison  344.600005   15.0   \n",
       "3        1999        00-0005883            E.George  287.800003   15.0   \n",
       "4        1999        00-0015218             J.Smith  285.100003   15.0   \n",
       "...       ...               ...                 ...         ...    ...   \n",
       "13979    2024        00-0039139             J.Gibbs  306.800002   16.0   \n",
       "13980    2024        00-0032764             D.Henry  306.300003   16.0   \n",
       "13981    2024        00-0039040            D.Achane  281.800002   16.0   \n",
       "13982    2024        00-0035700            J.Jacobs  281.500002   16.0   \n",
       "13983    2024        00-0037840          K.Williams  272.100003   16.0   \n",
       "\n",
       "             ppg  \n",
       "0      23.840000  \n",
       "1      25.335715  \n",
       "2      22.973334  \n",
       "3      19.186667  \n",
       "4      19.006667  \n",
       "...          ...  \n",
       "13979  19.175000  \n",
       "13980  19.143750  \n",
       "13981  17.612500  \n",
       "13982  17.593750  \n",
       "13983  17.006250  \n",
       "\n",
       "[260 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macsa\\anaconda3\\Lib\\site-packages\\IPython\\core\\formatters.py:347: FutureWarning: Index.format is deprecated and will be removed in a future version. Convert using index.astype(str) or index.map(formatter) instead.\n",
      "  return method()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>team</th>\n",
       "      <th>dst_fp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999</td>\n",
       "      <td>STL</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999</td>\n",
       "      <td>JAX</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999</td>\n",
       "      <td>KC</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999</td>\n",
       "      <td>DAL</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999</td>\n",
       "      <td>BAL</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>2024</td>\n",
       "      <td>SEA</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>2024</td>\n",
       "      <td>BUF</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>2024</td>\n",
       "      <td>PHI</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>2024</td>\n",
       "      <td>DAL</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>2024</td>\n",
       "      <td>DET</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     season team  dst_fp\n",
       "0      1999  STL     236\n",
       "1      1999  JAX     212\n",
       "2      1999   KC     197\n",
       "3      1999  DAL     190\n",
       "4      1999  BAL     181\n",
       "..      ...  ...     ...\n",
       "802    2024  SEA     138\n",
       "803    2024  BUF     136\n",
       "804    2024  PHI     136\n",
       "805    2024  DAL     127\n",
       "806    2024  DET     127\n",
       "\n",
       "[260 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macsa\\anaconda3\\Lib\\site-packages\\IPython\\core\\formatters.py:347: FutureWarning: Index.format is deprecated and will be removed in a future version. Convert using index.astype(str) or index.map(formatter) instead.\n",
      "  return method()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>kicker_player_id</th>\n",
       "      <th>kicker_player_name</th>\n",
       "      <th>posteam</th>\n",
       "      <th>total_fp</th>\n",
       "      <th>games</th>\n",
       "      <th>fg_made</th>\n",
       "      <th>fg_att</th>\n",
       "      <th>pat_made</th>\n",
       "      <th>pat_att</th>\n",
       "      <th>ppg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999</td>\n",
       "      <td>00-0010373</td>\n",
       "      <td>O.Mare</td>\n",
       "      <td>MIA</td>\n",
       "      <td>148</td>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>45.0</td>\n",
       "      <td>26</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999</td>\n",
       "      <td>00-0016830</td>\n",
       "      <td>M.Vanderjagt</td>\n",
       "      <td>IND</td>\n",
       "      <td>147</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>36.0</td>\n",
       "      <td>43</td>\n",
       "      <td>43.0</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999</td>\n",
       "      <td>00-0012875</td>\n",
       "      <td>T.Peterson</td>\n",
       "      <td>SEA</td>\n",
       "      <td>133</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999</td>\n",
       "      <td>00-0007622</td>\n",
       "      <td>M.Hollis</td>\n",
       "      <td>JAX</td>\n",
       "      <td>131</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>35.0</td>\n",
       "      <td>34</td>\n",
       "      <td>34.0</td>\n",
       "      <td>8.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999</td>\n",
       "      <td>00-0004811</td>\n",
       "      <td>J.Elam</td>\n",
       "      <td>DEN</td>\n",
       "      <td>119</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>34.0</td>\n",
       "      <td>29</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>2024</td>\n",
       "      <td>00-0035358</td>\n",
       "      <td>C.McLaughlin</td>\n",
       "      <td>TB</td>\n",
       "      <td>151</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>30.0</td>\n",
       "      <td>51</td>\n",
       "      <td>53.0</td>\n",
       "      <td>9.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>2024</td>\n",
       "      <td>00-0039172</td>\n",
       "      <td>J.Bates</td>\n",
       "      <td>DET</td>\n",
       "      <td>150</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>28.0</td>\n",
       "      <td>60</td>\n",
       "      <td>63.0</td>\n",
       "      <td>9.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>2024</td>\n",
       "      <td>00-0032569</td>\n",
       "      <td>W.Lutz</td>\n",
       "      <td>DEN</td>\n",
       "      <td>149</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>33.0</td>\n",
       "      <td>41</td>\n",
       "      <td>41.0</td>\n",
       "      <td>9.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>2024</td>\n",
       "      <td>00-0036162</td>\n",
       "      <td>T.Bass</td>\n",
       "      <td>BUF</td>\n",
       "      <td>137</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>28.0</td>\n",
       "      <td>58</td>\n",
       "      <td>62.0</td>\n",
       "      <td>8.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>2024</td>\n",
       "      <td>00-0029597</td>\n",
       "      <td>J.Tucker</td>\n",
       "      <td>BAL</td>\n",
       "      <td>130</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>30.0</td>\n",
       "      <td>55</td>\n",
       "      <td>57.0</td>\n",
       "      <td>8.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      season kicker_player_id kicker_player_name posteam  total_fp  games   \n",
       "0       1999       00-0010373             O.Mare     MIA       148     15  \\\n",
       "1       1999       00-0016830       M.Vanderjagt     IND       147     15   \n",
       "2       1999       00-0012875         T.Peterson     SEA       133     15   \n",
       "3       1999       00-0007622           M.Hollis     JAX       131     15   \n",
       "4       1999       00-0004811             J.Elam     DEN       119     15   \n",
       "...      ...              ...                ...     ...       ...    ...   \n",
       "1083    2024       00-0035358       C.McLaughlin      TB       151     16   \n",
       "1084    2024       00-0039172            J.Bates     DET       150     16   \n",
       "1085    2024       00-0032569             W.Lutz     DEN       149     16   \n",
       "1086    2024       00-0036162             T.Bass     BUF       137     16   \n",
       "1087    2024       00-0029597           J.Tucker     BAL       130     16   \n",
       "\n",
       "      fg_made  fg_att  pat_made  pat_att       ppg  \n",
       "0          38    45.0        26     26.0  9.866667  \n",
       "1          32    36.0        43     43.0  9.800000  \n",
       "2          31    37.0        32     32.0  8.866667  \n",
       "3          30    35.0        34     34.0  8.733333  \n",
       "4          27    34.0        29     29.0  7.933333  \n",
       "...       ...     ...       ...      ...       ...  \n",
       "1083       28    30.0        51     53.0  9.437500  \n",
       "1084       25    28.0        60     63.0  9.375000  \n",
       "1085       30    33.0        41     41.0  9.312500  \n",
       "1086       23    28.0        58     62.0  8.562500  \n",
       "1087       22    30.0        55     57.0  8.125000  \n",
       "\n",
       "[260 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Fantasy PPR Scoring with nflreadpy loader (Pandas pipeline) ===\n",
    "# - Loads each season via nflreadpy (Polars) â†’ converts to pandas\n",
    "# - Applies existing filters & ESPN-standard PPR scoring\n",
    "# - Includes historical team normalization and week caps by era\n",
    "# - Produces multi-season leaders/weekly (players), D/ST, and kicker tables\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nflreadpy as nfl\n",
    "\n",
    "# ----------------------------------\n",
    "# Config\n",
    "# ----------------------------------\n",
    "ALL_SEASONS = list(range(1999, 2025))  # 1999â€“2023 in one pass\n",
    "\n",
    "def fantasy_week_max(season: int) -> int:\n",
    "    # Common fantasy policy: stop before NFL's final week\n",
    "    # <=2020: Week 16; >=2021: Week 17\n",
    "    return 16 if season <= 2020 else 17\n",
    "\n",
    "# ----------------------------------\n",
    "# Team normalization (historical)\n",
    "# ----------------------------------\n",
    "# Normalize to SEASON-ACCURATE abbreviations:\n",
    "# - Rams:   STL through 2015, LAR 2016+\n",
    "# - Raiders: OAK through 2019, LV 2020+\n",
    "# - Chargers: SD through 2016, LAC 2017+\n",
    "# Everything else: pass-through\n",
    "def normalize_team(team: str, season: int) -> str:\n",
    "    t = (team or \"\").upper()\n",
    "    # Rams\n",
    "    if season <= 2015 and t in {\"LAR\", \"LA\", \"STL\"}:\n",
    "        return \"STL\"\n",
    "    if season >= 2016 and t in {\"LAR\", \"LA\", \"STL\"}:\n",
    "        return \"LAR\"\n",
    "    # Raiders\n",
    "    if season <= 2019 and t in {\"OAK\", \"LV\"}:\n",
    "        return \"OAK\"\n",
    "    if season >= 2020 and t in {\"OAK\", \"LV\"}:\n",
    "        return \"LV\"\n",
    "    # Chargers\n",
    "    if season <= 2016 and t in {\"SD\", \"LAC\"}:\n",
    "        return \"SD\"\n",
    "    if season >= 2017 and t in {\"SD\", \"LAC\"}:\n",
    "        return \"LAC\"\n",
    "    return t\n",
    "\n",
    "# ----------------------------------\n",
    "# Utilities\n",
    "# ----------------------------------\n",
    "def base_filter(df: pd.DataFrame, week_min: int, week_max: int) -> pd.DataFrame:\n",
    "    out = df.query(\"season_type == 'REG' and @week_min <= week <= @week_max\").copy()\n",
    "    if \"play_deleted\" in out.columns:\n",
    "        out = out[out[\"play_deleted\"].fillna(0) != 1]\n",
    "    if \"play_type\" in out.columns:\n",
    "        out = out[out[\"play_type\"] != \"no_play\"]\n",
    "    return out\n",
    "\n",
    "def safe_col(df, name, default=0):\n",
    "    return df[name].fillna(0) if name in df.columns else 0\n",
    "\n",
    "# ----------------------------------\n",
    "# 1) Player PPR (ESPN-standard)\n",
    "# ----------------------------------\n",
    "def player_ppr(pbp_reg: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = pbp_reg.copy()\n",
    "\n",
    "    pass_attempt   = (df.get(\"pass_attempt\", df.get(\"pass\", 0)).fillna(0).astype(int) == 1)\n",
    "    rush_attempt   = (df.get(\"rush_attempt\", df.get(\"rush\", 0)).fillna(0).astype(int) == 1)\n",
    "    complete_pass  = (df.get(\"complete_pass\", 0).fillna(0).astype(int) == 1)\n",
    "\n",
    "    pass_role = pass_attempt  & (df[\"fantasy_player_id\"] == df[\"passer_player_id\"])\n",
    "    rush_role = rush_attempt  & (df[\"fantasy_player_id\"] == df[\"rusher_player_id\"])\n",
    "    recv_role = complete_pass & (df[\"fantasy_player_id\"] == df[\"receiver_player_id\"])\n",
    "\n",
    "    # Passing\n",
    "    pass_fp = np.where(pass_role,\n",
    "                       safe_col(df, \"passing_yards\")*0.04 + safe_col(df, \"pass_touchdown\")*4 - safe_col(df, \"interception\")*2,\n",
    "                       0)\n",
    "    # Rushing\n",
    "    rush_fp = np.where(rush_role,\n",
    "                       safe_col(df, \"rushing_yards\")*0.1 + safe_col(df, \"rush_touchdown\")*6,\n",
    "                       0)\n",
    "    # Receiving (PPR + yards + receiving TDs are flagged by pass_touchdown)\n",
    "    reception_pts = np.where(recv_role, 1, 0)\n",
    "    recv_yards_td = np.where(recv_role,\n",
    "                             safe_col(df, \"receiving_yards\")*0.1 + safe_col(df, \"pass_touchdown\")*6,\n",
    "                             0)\n",
    "\n",
    "    # 2-pt conversions: credit receiver on pass or rusher on rush (success only)\n",
    "    two_pt_success = (safe_col(df, \"two_point_attempt\") == 1) & (df.get(\"two_point_conv_result\", \"\") == \"success\")\n",
    "    two_pt_recv = two_pt_success & (safe_col(df, \"pass\") == 1) & (df[\"fantasy_player_id\"] == df[\"receiver_player_id\"])\n",
    "    two_pt_rush = two_pt_success & (safe_col(df, \"rush\") == 1) & (df[\"fantasy_player_id\"] == df[\"rusher_player_id\"])\n",
    "    two_pt_fp = np.where(two_pt_recv | two_pt_rush, 2, 0)\n",
    "\n",
    "    # Fumbles lost\n",
    "    fum_fp = np.where((safe_col(df, \"fumble_lost\") == 1) & (df[\"fantasy_player_id\"] == df.get(\"fumbled_1_player_id\")),\n",
    "                      -2, 0)\n",
    "\n",
    "    df[\"fp\"] = pass_fp + rush_fp + reception_pts + recv_yards_td + two_pt_fp + fum_fp\n",
    "\n",
    "    leaders = (df.groupby([\"season\",\"fantasy_player_id\",\"fantasy_player_name\"], as_index=False)[\"fp\"].sum()\n",
    "                 .sort_values([\"season\",\"fp\"], ascending=[True, False]))\n",
    "    games = (df.loc[df[\"fp\"] != 0]\n",
    "               .groupby([\"season\",\"fantasy_player_id\",\"fantasy_player_name\"], as_index=False)[\"week\"].nunique()\n",
    "               .rename(columns={\"week\":\"games\"}))\n",
    "    leaders = leaders.merge(games, on=[\"season\",\"fantasy_player_id\",\"fantasy_player_name\"], how=\"left\")\n",
    "    leaders[\"ppg\"] = leaders[\"fp\"] / leaders[\"games\"]\n",
    "\n",
    "    weekly = (df.groupby([\"season\",\"fantasy_player_id\",\"fantasy_player_name\",\"week\"], as_index=False)[\"fp\"].sum()\n",
    "                .sort_values([\"season\",\"fantasy_player_name\",\"week\"]))\n",
    "\n",
    "    return leaders, weekly\n",
    "\n",
    "# ----------------------------------\n",
    "# 2) Defense/Special Teams (D/ST)\n",
    "# ----------------------------------\n",
    "def dst_scoring(pbp_reg: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = pbp_reg.copy()\n",
    "\n",
    "    # Final scores + teams per game\n",
    "    scores = (df.groupby([\"season\",\"game_id\"], as_index=False)[[\"total_home_score\",\"total_away_score\",\"home_team\",\"away_team\"]]\n",
    "                .agg({\"total_home_score\":\"max\",\"total_away_score\":\"max\",\"home_team\":\"first\",\"away_team\":\"first\"})\n",
    "                .rename(columns={\"total_home_score\":\"home_pts_final\",\"total_away_score\":\"away_pts_final\"}))\n",
    "\n",
    "    home_rows = scores[[\"season\",\"game_id\",\"home_team\",\"away_pts_final\"]].rename(\n",
    "        columns={\"home_team\":\"team\",\"away_pts_final\":\"points_allowed\"})\n",
    "    away_rows = scores[[\"season\",\"game_id\",\"away_team\",\"home_pts_final\"]].rename(\n",
    "        columns={\"away_team\":\"team\",\"home_pts_final\":\"points_allowed\"})\n",
    "    pa_tbl = pd.concat([home_rows, away_rows], ignore_index=True)\n",
    "\n",
    "    # Points-allowed tiers (ESPN-ish)\n",
    "    def pa_to_pts(pa: int) -> int:\n",
    "        if pa == 0: return 10\n",
    "        if pa <= 6: return 7\n",
    "        if pa <= 13: return 4\n",
    "        if pa <= 17: return 1\n",
    "        if pa <= 27: return 0\n",
    "        if pa <= 34: return -1\n",
    "        if pa <= 45: return -4\n",
    "        return -5\n",
    "\n",
    "    pa_tbl[\"pa_pts\"] = pa_tbl[\"points_allowed\"].astype(int).map(pa_to_pts)\n",
    "\n",
    "    # Tallies by defense\n",
    "    by_def = (df.groupby([\"season\",\"game_id\",\"defteam\"], as_index=False)\n",
    "                .agg({\"sack\":\"sum\",\"interception\":\"sum\",\"safety\":\"sum\",\"punt_blocked\":\"sum\"})\n",
    "                .rename(columns={\"defteam\":\"team\"}))\n",
    "\n",
    "    # Blocked FGs\n",
    "    tmp = df.copy()\n",
    "    tmp[\"fg_block\"] = (tmp.get(\"field_goal_result\",\"\").astype(str).str.lower() == \"blocked\").astype(int)\n",
    "    by_def_fgblk = (tmp.groupby([\"season\",\"game_id\",\"defteam\"], as_index=False)[\"fg_block\"].sum()\n",
    "                      .rename(columns={\"defteam\":\"team\"}))\n",
    "\n",
    "    # Fumble recoveries credited to recovering team\n",
    "    fr_parts = []\n",
    "    if \"fumble_recovery_1_team\" in df.columns:\n",
    "        fr1 = df[df[\"fumble_recovery_1_team\"].notna()][[\"season\",\"game_id\",\"fumble_recovery_1_team\"]].copy()\n",
    "        fr1[\"team\"] = fr1[\"fumble_recovery_1_team\"]; fr1[\"fr\"] = 1\n",
    "        fr_parts.append(fr1[[\"season\",\"game_id\",\"team\",\"fr\"]])\n",
    "    if \"fumble_recovery_2_team\" in df.columns:\n",
    "        fr2 = df[df[\"fumble_recovery_2_team\"].notna()][[\"season\",\"game_id\",\"fumble_recovery_2_team\"]].copy()\n",
    "        fr2[\"team\"] = fr2[\"fumble_recovery_2_team\"]; fr2[\"fr\"] = 1\n",
    "        fr_parts.append(fr2[[\"season\",\"game_id\",\"team\",\"fr\"]])\n",
    "    by_fr = (pd.concat(fr_parts, ignore_index=True)\n",
    "             .groupby([\"season\",\"game_id\",\"team\"], as_index=False)[\"fr\"].sum()) if fr_parts else \\\n",
    "            pd.DataFrame(columns=[\"season\",\"game_id\",\"team\",\"fr\"])\n",
    "\n",
    "    # Return TDs (credited to return team)\n",
    "    ret = df[(safe_col(df,\"return_touchdown\") == 1)]\n",
    "    by_ret_td = (ret.groupby([\"season\",\"game_id\",\"return_team\"], as_index=False).size()\n",
    "                   .rename(columns={\"return_team\":\"team\",\"size\":\"ret_td\"}))\n",
    "\n",
    "    # Merge all components\n",
    "    dst = pa_tbl[[\"season\",\"game_id\",\"team\",\"pa_pts\"]].copy()\n",
    "    for part in [\n",
    "        by_def[[\"season\",\"game_id\",\"team\",\"sack\",\"interception\",\"safety\",\"punt_blocked\"]],\n",
    "        by_def_fgblk[[\"season\",\"game_id\",\"team\",\"fg_block\"]],\n",
    "        by_fr[[\"season\",\"game_id\",\"team\",\"fr\"]],\n",
    "        by_ret_td[[\"season\",\"game_id\",\"team\",\"ret_td\"]],\n",
    "    ]:\n",
    "        dst = dst.merge(part, on=[\"season\",\"game_id\",\"team\"], how=\"left\")\n",
    "\n",
    "    dst = dst.fillna(0)\n",
    "    for c in [\"sack\",\"interception\",\"safety\",\"punt_blocked\",\"fg_block\",\"fr\",\"ret_td\"]:\n",
    "        if c in dst.columns: dst[c] = dst[c].astype(int)\n",
    "\n",
    "    # Normalize historical team code\n",
    "    dst[\"team\"] = [normalize_team(t, s) for t, s in zip(dst[\"team\"].astype(str), dst[\"season\"].astype(int))]\n",
    "\n",
    "    dst[\"dst_fp\"] = (\n",
    "        dst[\"sack\"]*1 + dst[\"interception\"]*2 + dst[\"fr\"]*2 + dst[\"safety\"]*2\n",
    "        + dst[\"ret_td\"]*6 + (dst[\"punt_blocked\"] + dst[\"fg_block\"])*2 + dst[\"pa_pts\"]\n",
    "    )\n",
    "\n",
    "    per_game = dst.sort_values([\"season\",\"game_id\",\"team\"]).reset_index(drop=True)\n",
    "    season = (per_game.groupby([\"season\",\"team\"], as_index=False)[\"dst_fp\"].sum()\n",
    "                      .sort_values([\"season\",\"dst_fp\"], ascending=[True, False]))\n",
    "    return season, per_game\n",
    "\n",
    "# ----------------------------------\n",
    "# 3) Kickers (ESPN-style)\n",
    "# ----------------------------------\n",
    "def kicker_scoring(pbp_reg: pd.DataFrame, miss_pat_minus_one=False) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = pbp_reg.copy()\n",
    "    kick = df[(safe_col(df, \"field_goal_attempt\") == 1) | (safe_col(df, \"extra_point_attempt\") == 1)].copy()\n",
    "\n",
    "    for c in [\"field_goal_result\",\"extra_point_result\"]:\n",
    "        if c in kick.columns: \n",
    "            kick[c] = kick[c].astype(str).str.lower()\n",
    "\n",
    "    # FG made by distance\n",
    "    fg_made = (safe_col(kick, \"field_goal_attempt\") == 1) & (kick[\"field_goal_result\"] == \"made\")\n",
    "    dist = safe_col(kick, \"kick_distance\").astype(float)\n",
    "    fg_pts = np.where(fg_made & (dist >= 60), 6,\n",
    "              np.where(fg_made & (dist >= 50), 5,\n",
    "              np.where(fg_made & (dist >= 40), 4,\n",
    "              np.where(fg_made, 3, 0))))\n",
    "\n",
    "    # Missed FG (includes blocked)\n",
    "    fg_miss = (safe_col(kick, \"field_goal_attempt\") == 1) & (kick[\"field_goal_result\"].isin([\"missed\",\"blocked\"]))\n",
    "    fg_miss_pts = np.where(fg_miss, -1, 0)\n",
    "\n",
    "    # PATs\n",
    "    pat_attempt = (safe_col(kick, \"extra_point_attempt\") == 1)\n",
    "    pat_good = pat_attempt & (kick[\"extra_point_result\"].isin([\"good\",\"made\"]))\n",
    "    pat_pts = np.where(pat_good, 1, 0)\n",
    "    pat_missed = pat_attempt & (kick[\"extra_point_result\"].isin([\"failed\",\"missed\",\"blocked\"]))\n",
    "    pat_miss_pts = np.where(miss_pat_minus_one & pat_missed, -1, 0)\n",
    "\n",
    "    kick[\"k_fp\"] = fg_pts + fg_miss_pts + pat_pts + pat_miss_pts\n",
    "\n",
    "    id_col = \"kicker_player_id\" if \"kicker_player_id\" in kick.columns else \"fantasy_player_id\"\n",
    "    name_col = \"kicker_player_name\" if \"kicker_player_name\" in kick.columns else \"fantasy_player_name\"\n",
    "\n",
    "    per_game = (kick.groupby([\"season\",\"game_id\", id_col, name_col, \"posteam\"], as_index=False)\n",
    "                  .agg(k_fp=(\"k_fp\",\"sum\"),\n",
    "                       fg_made=(\"field_goal_attempt\", lambda s: int(((s==1) & (kick.loc[s.index,'field_goal_result'].eq('made'))).sum())),\n",
    "                       fg_att=(\"field_goal_attempt\",\"sum\"),\n",
    "                       pat_made=(\"extra_point_attempt\", lambda s: int(((s==1) & (kick.loc[s.index,'extra_point_result'].isin(['good','made']))).sum())),\n",
    "                       pat_att=(\"extra_point_attempt\",\"sum\"))\n",
    "               )\n",
    "\n",
    "    # Normalize team code historically for posteam in outputs\n",
    "    per_game[\"posteam\"] = [normalize_team(t, s) for t, s in zip(per_game[\"posteam\"].astype(str), per_game[\"season\"].astype(int))]\n",
    "\n",
    "    season = (per_game.groupby([\"season\", id_col, name_col, \"posteam\"], as_index=False)\n",
    "                      .agg(total_fp=(\"k_fp\",\"sum\"),\n",
    "                           games=(\"game_id\",\"nunique\"),\n",
    "                           fg_made=(\"fg_made\",\"sum\"),\n",
    "                           fg_att=(\"fg_att\",\"sum\"),\n",
    "                           pat_made=(\"pat_made\",\"sum\"),\n",
    "                           pat_att=(\"pat_att\",\"sum\")))\n",
    "    season[\"ppg\"] = season[\"total_fp\"] / season[\"games\"]\n",
    "    season = season.sort_values([\"season\",\"total_fp\",\"ppg\"], ascending=[True, False, False])\n",
    "    return season, per_game\n",
    "\n",
    "# ----------------------------------\n",
    "# Runner (streams seasons safely) â€” uses nflreadpy loader\n",
    "# ----------------------------------\n",
    "players_all, players_weekly_all = [], []\n",
    "dst_all, dst_games_all = [], []\n",
    "k_all, k_games_all = [], []\n",
    "\n",
    "for yr in ALL_SEASONS:\n",
    "    print(f\"Processing {yr} ...\")\n",
    "    # Load via nflreadpy (Polars) then convert to pandas\n",
    "    pbp_pl = nfl.load_pbp([yr])      # Polars DataFrame\n",
    "    pbp_y  = pbp_pl.to_pandas()      # hand off to pandas pipeline\n",
    "\n",
    "    wk_max = fantasy_week_max(yr)\n",
    "    pbp_reg = base_filter(pbp_y, week_min=1, week_max=wk_max)\n",
    "\n",
    "    # Optional RAM saver\n",
    "    for c in pbp_reg.select_dtypes(\"float64\").columns:\n",
    "        pbp_reg[c] = pbp_reg[c].astype(\"float32\")\n",
    "    print(f\"{yr} done.\\nDowncasting floats.\")\n",
    "\n",
    "    # Players\n",
    "    p_season, p_weekly = player_ppr(pbp_reg)\n",
    "    players_all.append(p_season)\n",
    "    players_weekly_all.append(p_weekly)\n",
    "\n",
    "    # D/ST\n",
    "    d_season, d_games = dst_scoring(pbp_reg)\n",
    "    dst_all.append(d_season)\n",
    "    dst_games_all.append(d_games)\n",
    "\n",
    "    # Kickers\n",
    "    k_season, k_games = kicker_scoring(pbp_reg, miss_pat_minus_one=False)\n",
    "    k_all.append(k_season)\n",
    "    k_games_all.append(k_games)\n",
    "\n",
    "# Concatenate multi-season outputs\n",
    "players_leaders_multi = pd.concat(players_all, ignore_index=True)\n",
    "players_weekly_multi  = pd.concat(players_weekly_all, ignore_index=True)\n",
    "dst_season_multi      = pd.concat(dst_all, ignore_index=True)\n",
    "dst_per_game_multi    = pd.concat(dst_games_all, ignore_index=True)\n",
    "k_season_multi        = pd.concat(k_all, ignore_index=True)\n",
    "k_per_game_multi      = pd.concat(k_games_all, ignore_index=True)\n",
    "\n",
    "# Examples:\n",
    "display(players_leaders_multi.groupby(\"season\").head(10))\n",
    "display(dst_season_multi.groupby(\"season\").head(10))\n",
    "display(k_season_multi.groupby(\"season\").head(10))\n",
    "\n",
    "# (Optional) Save to files for dashboards\n",
    "# players_leaders_multi.to_parquet(\"players_ppr_allseasons.parquet\", index=False)\n",
    "# players_weekly_multi.to_parquet(\"players_weekly_ppr_allseasons.parquet\", index=False)\n",
    "# dst_season_multi.to_parquet(\"dst_season_allseasons.parquet\", index=False)\n",
    "# dst_per_game_multi.to_parquet(\"dst_per_game_allseasons.parquet\", index=False)\n",
    "# k_season_multi.to_parquet(\"kickers_season_allseasons.parquet\", index=False)\n",
    "# k_per_game_multi.to_parquet(\"kickers_per_game_allseasons.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff75b4b6-7e4d-42b1-92aa-ea9628c8f549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[team map] loading 1999 ...\n",
      "[team map] loading 2000 ...\n",
      "[team map] loading 2001 ...\n",
      "[team map] loading 2002 ...\n",
      "[team map] loading 2003 ...\n",
      "[team map] loading 2004 ...\n",
      "[team map] loading 2005 ...\n",
      "[team map] loading 2006 ...\n",
      "[team map] loading 2007 ...\n",
      "[team map] loading 2008 ...\n",
      "[team map] loading 2009 ...\n",
      "[team map] loading 2010 ...\n",
      "[team map] loading 2011 ...\n",
      "[team map] loading 2012 ...\n",
      "[team map] loading 2013 ...\n",
      "[team map] loading 2014 ...\n",
      "[team map] loading 2015 ...\n",
      "[team map] loading 2016 ...\n",
      "[team map] loading 2017 ...\n",
      "[team map] loading 2018 ...\n",
      "[team map] loading 2019 ...\n",
      "[team map] loading 2020 ...\n",
      "[team map] loading 2021 ...\n",
      "[team map] loading 2022 ...\n",
      "[team map] loading 2023 ...\n",
      "[team map] loading 2024 ...\n",
      "[pos map] loading 1999 ...\n",
      "[pos map] loading 2000 ...\n",
      "[pos map] loading 2001 ...\n",
      "[pos map] loading 2002 ...\n",
      "[pos map] loading 2003 ...\n",
      "[pos map] loading 2004 ...\n",
      "[pos map] loading 2005 ...\n",
      "[pos map] loading 2006 ...\n",
      "[pos map] loading 2007 ...\n",
      "[pos map] loading 2008 ...\n",
      "[pos map] loading 2009 ...\n",
      "[pos map] loading 2010 ...\n",
      "[pos map] loading 2011 ...\n",
      "[pos map] loading 2012 ...\n",
      "[pos map] loading 2013 ...\n",
      "[pos map] loading 2014 ...\n",
      "[pos map] loading 2015 ...\n",
      "[pos map] loading 2016 ...\n",
      "[pos map] loading 2017 ...\n",
      "[pos map] loading 2018 ...\n",
      "[pos map] loading 2019 ...\n",
      "[pos map] loading 2020 ...\n",
      "[pos map] loading 2021 ...\n",
      "[pos map] loading 2022 ...\n",
      "[pos map] loading 2023 ...\n",
      "[pos map] loading 2024 ...\n",
      "players_weekly_with_ctx columns: ['season', 'player_id', 'player_name', 'week', 'fp', 'team', 'position'] ...\n",
      "Sample:\n",
      "  season  player_id player_name  week        fp team position\n",
      "   1999 00-0003378   A.Connell     1 24.500000  WAS       WR\n",
      "   1999 00-0003378   A.Connell     2  6.900000  WAS       WR\n",
      "   1999 00-0003378   A.Connell     3 12.500000  WAS       WR\n",
      "   1999 00-0003378   A.Connell     4 30.400001  WAS       WR\n",
      "   1999 00-0003378   A.Connell     6 19.000000  WAS       WR\n",
      "   1999 00-0003378   A.Connell     7 13.200000  WAS       WR\n",
      "   1999 00-0003378   A.Connell     8  6.600000  WAS       WR\n",
      "   1999 00-0003378   A.Connell     9 18.400000  WAS       WR\n"
     ]
    }
   ],
   "source": [
    "# --- Build (season, week, player_id) -> team, position lookup and merge into players_weekly_multi ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nflreadpy as nfl\n",
    "\n",
    "# 1) Minimal per-play load to infer TEAM per player-week (mode of posteam where player was involved)\n",
    "def build_player_week_team(seasons):\n",
    "    parts = []\n",
    "    for yr in seasons:\n",
    "        print(f\"[team map] loading {yr} ...\")\n",
    "        df = nfl.load_pbp([yr]).to_pandas()\n",
    "        # keep light\n",
    "        keep = [\"season\",\"week\",\"posteam\",\"fantasy_player_id\",\n",
    "                \"pass_attempt\",\"rush_attempt\",\"complete_pass\",\n",
    "                \"passer_player_id\",\"rusher_player_id\",\"receiver_player_id\"]\n",
    "        keep = [c for c in keep if c in df.columns]\n",
    "        df = df[keep].copy()\n",
    "        df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "        # where did the fantasy player \"act\" on the play?\n",
    "        pass_attempt = (df.get(\"pass_attempt\", 0).fillna(0).astype(int) == 1)\n",
    "        rush_attempt = (df.get(\"rush_attempt\", 0).fillna(0).astype(int) == 1)\n",
    "        complete_pass = (df.get(\"complete_pass\", 0).fillna(0).astype(int) == 1)\n",
    "\n",
    "        is_actor = (\n",
    "            (pass_attempt & (df[\"fantasy_player_id\"] == df.get(\"passer_player_id\"))) |\n",
    "            (rush_attempt & (df[\"fantasy_player_id\"] == df.get(\"rusher_player_id\"))) |\n",
    "            (complete_pass & (df[\"fantasy_player_id\"] == df.get(\"receiver_player_id\")))\n",
    "        )\n",
    "\n",
    "        act = df.loc[is_actor, [\"season\",\"week\",\"posteam\",\"fantasy_player_id\"]].dropna(subset=[\"fantasy_player_id\"])\n",
    "        if \"posteam\" not in act.columns:\n",
    "            act[\"posteam\"] = np.nan\n",
    "\n",
    "        # team per (season,week,player): mode of posteam\n",
    "        team_mode = (act.groupby([\"season\",\"week\",\"fantasy_player_id\",\"posteam\"], as_index=False)\n",
    "                        .size()\n",
    "                        .sort_values([\"season\",\"week\",\"fantasy_player_id\",\"size\"], ascending=[True,True,True,False]))\n",
    "        team_mode = team_mode.drop_duplicates([\"season\",\"week\",\"fantasy_player_id\"]).rename(columns={\"fantasy_player_id\":\"player_id\",\"posteam\":\"team\"})\n",
    "        parts.append(team_mode[[\"season\",\"week\",\"player_id\",\"team\"]])\n",
    "    team_map = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame(columns=[\"season\",\"week\",\"player_id\",\"team\"])\n",
    "    return team_map\n",
    "\n",
    "# 2) Position map â€” prefer roster if available; fallback to role inference per season\n",
    "def build_player_position_map(seasons):\n",
    "    pos_parts = []\n",
    "    for yr in seasons:\n",
    "        print(f\"[pos map] loading {yr} ...\")\n",
    "        df = nfl.load_pbp([yr]).to_pandas()\n",
    "        keep = [\"season\",\"fantasy_player_id\",\n",
    "                \"pass_attempt\",\"rush_attempt\",\"complete_pass\",\n",
    "                \"passer_player_id\",\"rusher_player_id\",\"receiver_player_id\"]\n",
    "        keep = [c for c in keep if c in df.columns]\n",
    "        df = df[keep].copy()\n",
    "        df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "        # counts by role per player-season\n",
    "        df[\"is_pass\"] = ((df.get(\"pass_attempt\",0)==1) & (df[\"fantasy_player_id\"] == df.get(\"passer_player_id\"))).astype(int)\n",
    "        df[\"is_rush\"] = ((df.get(\"rush_attempt\",0)==1) & (df[\"fantasy_player_id\"] == df.get(\"rusher_player_id\"))).astype(int)\n",
    "        df[\"is_recv\"] = ((df.get(\"complete_pass\",0)==1) & (df[\"fantasy_player_id\"] == df.get(\"receiver_player_id\"))).astype(int)\n",
    "\n",
    "        grp = (df.groupby([\"season\",\"fantasy_player_id\"], as_index=False)[[\"is_pass\",\"is_rush\",\"is_recv\"]].sum()\n",
    "                 .rename(columns={\"fantasy_player_id\":\"player_id\"}))\n",
    "\n",
    "        # heuristics if roster not used:\n",
    "        # mostly passer => QB; mostly rush > recv => RB; else pass-catcher => WR (could be TE; you can refine with a roster later)\n",
    "        def infer_pos(row):\n",
    "            if row[\"is_pass\"] >= max(row[\"is_rush\"], row[\"is_recv\"]): return \"QB\"\n",
    "            if row[\"is_rush\"] > row[\"is_recv\"]: return \"RB\"\n",
    "            return \"WR\"  # WR/TE bucket; refine with roster if needed\n",
    "\n",
    "        grp[\"position\"] = grp.apply(infer_pos, axis=1)\n",
    "        pos_parts.append(grp[[\"season\",\"player_id\",\"position\"]])\n",
    "    pos_map = pd.concat(pos_parts, ignore_index=True) if pos_parts else pd.DataFrame(columns=[\"season\",\"player_id\",\"position\"])\n",
    "    return pos_map\n",
    "\n",
    "# Build maps just for seasons present in your weekly table\n",
    "_seasons = sorted(players_weekly_multi[\"season\"].unique().tolist())\n",
    "team_map = build_player_week_team(_seasons)\n",
    "pos_map  = build_player_position_map(_seasons)\n",
    "\n",
    "# Merge maps into players_weekly_multi\n",
    "pw = players_weekly_multi.copy().rename(columns={\n",
    "    \"fantasy_player_id\":\"player_id\",\n",
    "    \"fantasy_player_name\":\"player_name\"\n",
    "})\n",
    "pw = pw.merge(team_map, on=[\"season\",\"week\",\"player_id\"], how=\"left\")\n",
    "pw = pw.merge(pos_map,  on=[\"season\",\"player_id\"],            how=\"left\")\n",
    "\n",
    "# Final sanity: fill missing team/position if any\n",
    "pw[\"team\"] = pw[\"team\"].fillna(\"UNK\")\n",
    "pw[\"position\"] = pw[\"position\"].fillna(\"WR\")  # neutral default for pass-catcher\n",
    "\n",
    "# Expose pw for downstream feature builder\n",
    "players_weekly_with_ctx = pw\n",
    "\n",
    "print(\"players_weekly_with_ctx columns:\", players_weekly_with_ctx.columns.tolist()[:25], \"...\")\n",
    "print(\"Sample:\\n\", players_weekly_with_ctx.head(8).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f09a35d-9e01-4d3a-88ff-08020dbbedc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skill positions â€” shapes: train=(107833, 13), val=(5000, 13), test=(4935, 13)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- helpers ----\n",
    "def exists(df, col): \n",
    "    return col in df.columns\n",
    "\n",
    "def add_team_week_aggregates(pwk: pd.DataFrame):\n",
    "    \"\"\"Compute team totals per season-week to enable shares, then merge back.\"\"\"\n",
    "    need = [\"targets\",\"receptions\",\"carries\",\"receiving_yards\",\"rushing_yards\",\"air_yards\",\"fantasy_player_id\"]\n",
    "    have = [c for c in need if exists(pwk, c)]\n",
    "    base = pwk[[\"season\",\"week\",\"team\"] + have].copy()\n",
    "    team_tot = base.groupby([\"season\",\"week\",\"team\"], as_index=False).sum()\n",
    "    rename = {c: f\"team_{c}\" for c in have if c != \"fantasy_player_id\"}\n",
    "    team_tot = team_tot.rename(columns=rename).drop(columns=[c for c in team_tot.columns if c.endswith(\"fantasy_player_id\")], errors=\"ignore\")\n",
    "    return pwk.merge(team_tot, on=[\"season\",\"week\",\"team\"], how=\"left\")\n",
    "\n",
    "def add_shifted_rolls(df, by, cols, windows=(3,), prefix=\"r\"):\n",
    "    \"\"\"For each col, add shifted rolling means/sums grouped by keys in `by`.\"\"\"\n",
    "    df = df.sort_values(by + [\"season\",\"week\"])\n",
    "    for c in cols:\n",
    "        if not exists(df, c): \n",
    "            continue\n",
    "        for w in windows:\n",
    "            roll = (df.groupby(by)[c]\n",
    "                      .shift(1)  # avoid leakage\n",
    "                      .rolling(w, min_periods=1)\n",
    "                      .mean())\n",
    "            df[f\"{prefix}_{c}_{w}\"] = roll.values\n",
    "    return df\n",
    "\n",
    "def season_splits(df, season_col=\"season\", test_season=2024, val_season=2023):\n",
    "    tr = df[df[season_col] < val_season].copy()\n",
    "    va = df[df[season_col] == val_season].copy()\n",
    "    te = df[df[season_col] == test_season].copy()\n",
    "    return tr, va, te\n",
    "\n",
    "# ---- Skill positions table ----\n",
    "pw = players_weekly_with_ctx.copy()\n",
    "\n",
    "# unify column names we rely on\n",
    "rename_map = {\n",
    "    \"fantasy_player_id\":\"player_id\",\n",
    "    \"fantasy_player_name\":\"player_name\"\n",
    "}\n",
    "pw = pw.rename(columns={k:v for k,v in rename_map.items() if k in pw.columns})\n",
    "\n",
    "# ensure required keys\n",
    "need_keys = [\"season\",\"week\",\"player_id\",\"player_name\"]\n",
    "for k in need_keys:\n",
    "    if k not in pw.columns:\n",
    "        raise ValueError(f\"Missing required column in players_weekly_multi: {k}\")\n",
    "\n",
    "# team normalization already handled earlier; assume 'team' exists downstream. \n",
    "if \"team\" not in pw.columns and \"posteam\" in pw.columns:\n",
    "    pw[\"team\"] = pw[\"posteam\"]\n",
    "\n",
    "# basic feature cols (keep if present)\n",
    "base_feats = [\n",
    "    \"targets\",\"receptions\",\"carries\",\n",
    "    \"receiving_yards\",\"rushing_yards\",\n",
    "    \"passing_yards\",\"passing_tds\",\"rushing_tds\",\"receiving_tds\",\"interceptions\",\n",
    "    \"fumbles_lost\",\"air_yards\"\n",
    "]\n",
    "keep = [c for c in base_feats if c in pw.columns]\n",
    "fe = pw[[\"season\",\"week\",\"player_id\",\"player_name\",\"fp\",\"team\",\"position\"] + keep].copy()\n",
    "\n",
    "# shares (need team totals)\n",
    "fe = add_team_week_aggregates(fe)\n",
    "if \"targets\" in fe.columns and \"team_targets\" in fe.columns:\n",
    "    fe[\"target_share\"] = (fe[\"targets\"] / fe[\"team_targets\"]).fillna(0)\n",
    "if \"carries\" in fe.columns and \"team_carries\" in fe.columns:\n",
    "    fe[\"carry_share\"] = (fe[\"carries\"] / fe[\"team_carries\"]).fillna(0)\n",
    "if \"air_yards\" in fe.columns and \"team_air_yards\" in fe.columns:\n",
    "    fe[\"air_yards_share\"] = (fe[\"air_yards\"] / fe[\"team_air_yards\"]).fillna(0)\n",
    "\n",
    "# rolling player form/features (3,5,8 games)\n",
    "roll_cols = [c for c in [\"fp\",\"targets\",\"receptions\",\"carries\",\"receiving_yards\",\"rushing_yards\",\"passing_yards\"] if c in fe.columns]\n",
    "fe = add_shifted_rolls(fe, by=[\"player_id\"], cols=roll_cols, windows=(3,5,8), prefix=\"r\")\n",
    "\n",
    "# simple team form (rolling mean of total team points per week using players table as proxy)\n",
    "# If you have per-game final scores elsewhere, prefer that.\n",
    "if \"fp\" in fe.columns:\n",
    "    team_week_pts = fe.groupby([\"season\",\"week\",\"team\"], as_index=False)[\"fp\"].sum().rename(columns={\"fp\":\"team_fp_sum\"})\n",
    "    fe = fe.merge(team_week_pts, on=[\"season\",\"week\",\"team\"], how=\"left\")\n",
    "    fe = add_shifted_rolls(fe, by=[\"team\"], cols=[\"team_fp_sum\"], windows=(3,5), prefix=\"r\")\n",
    "\n",
    "# drop rows with missing target\n",
    "fe = fe.dropna(subset=[\"fp\"]).reset_index(drop=True)\n",
    "\n",
    "# split\n",
    "train_df, val_df, test_df = season_splits(fe, test_season=2024, val_season=2023)\n",
    "\n",
    "print(f\"Skill positions â€” shapes: train={train_df.shape}, val={val_df.shape}, test={test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4f4b9b8-30df-4f28-8a64-33a6d07d254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature counts â€” numeric: 5, categorical: 2\n",
      "Columns with NaNs (top 20):\n",
      "r_fp_3             88\n",
      "r_fp_5              2\n",
      "r_fp_8              1\n",
      "r_team_fp_sum_3     1\n",
      "r_team_fp_sum_5     1\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macsa\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macsa\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear [VAL]  MAE=4.184  RMSE=5.698  R2=0.390\n",
      "Linear [TEST]  MAE=4.263  RMSE=5.742  R2=0.401\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macsa\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macsa\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge [VAL]  MAE=4.184  RMSE=5.698  R2=0.390\n",
      "Ridge [TEST]  MAE=4.263  RMSE=5.741  R2=0.401\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m eval_model(ridge, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRidge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     80\u001b[0m rf \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprep\u001b[39m\u001b[38;5;124m\"\u001b[39m, pre), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreg\u001b[39m\u001b[38;5;124m\"\u001b[39m, RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))])\n\u001b[1;32m---> 81\u001b[0m eval_model(rf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandomForest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 65\u001b[0m, in \u001b[0;36meval_model\u001b[1;34m(pipe, name)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_model\u001b[39m(pipe, name):\n\u001b[1;32m---> 65\u001b[0m     pipe\u001b[38;5;241m.\u001b[39mfit(Xtr, ytr)\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m split, X, y \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVAL\u001b[39m\u001b[38;5;124m\"\u001b[39m, Xva, yva), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEST\u001b[39m\u001b[38;5;124m\"\u001b[39m, Xte, yte)]:\n\u001b[0;32m     67\u001b[0m         yhat \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    490\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    491\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    492\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    493\u001b[0m )(\n\u001b[0;32m    494\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    495\u001b[0m         t,\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    497\u001b[0m         X,\n\u001b[0;32m    498\u001b[0m         y,\n\u001b[0;32m    499\u001b[0m         sample_weight,\n\u001b[0;32m    500\u001b[0m         i,\n\u001b[0;32m    501\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    502\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    503\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    504\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    505\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    508\u001b[0m )\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- quick diagnostics: which columns have NaNs?\n",
    "def null_report(df, cols):\n",
    "    rep = pd.Series({c: df[c].isna().sum() for c in cols if c in df.columns})\n",
    "    rep = rep[rep > 0].sort_values(ascending=False)\n",
    "    if len(rep):\n",
    "        print(\"Columns with NaNs (top 20):\")\n",
    "        print(rep.head(20))\n",
    "    else:\n",
    "        print(\"No NaNs detected in selected columns.\")\n",
    "\n",
    "# choose columns actually present\n",
    "num_cols = [c for c in [\n",
    "    \"targets\",\"receptions\",\"carries\",\n",
    "    \"receiving_yards\",\"rushing_yards\",\n",
    "    \"passing_yards\",\"passing_tds\",\"rushing_tds\",\"receiving_tds\",\"interceptions\",\"fumbles_lost\",\"air_yards\",\n",
    "    \"target_share\",\"carry_share\",\"air_yards_share\",\n",
    "    \"r_fp_3\",\"r_fp_5\",\"r_fp_8\",\n",
    "    \"r_targets_3\",\"r_receptions_3\",\"r_carries_3\",\n",
    "    \"r_receiving_yards_3\",\"r_rushing_yards_3\",\"r_passing_yards_3\",\n",
    "    \"r_team_fp_sum_3\",\"r_team_fp_sum_5\"\n",
    "] if c in train_df.columns]\n",
    "\n",
    "cat_cols = [c for c in [\"team\",\"position\"] if c in train_df.columns]\n",
    "\n",
    "# bail out early if empty feature sets\n",
    "if not num_cols and not cat_cols:\n",
    "    raise ValueError(\"No features found in train_df. Check earlier feature-building steps.\")\n",
    "\n",
    "Xtr = train_df[num_cols + cat_cols].copy()\n",
    "ytr = train_df[\"fp\"].values\n",
    "Xva = val_df[num_cols + cat_cols].copy()\n",
    "yva = val_df[\"fp\"].values\n",
    "Xte = test_df[num_cols + cat_cols].copy()\n",
    "yte = test_df[\"fp\"].values\n",
    "\n",
    "print(f\"Feature counts â€” numeric: {len(num_cols)}, categorical: {len(cat_cols)}\")\n",
    "null_report(train_df, num_cols + cat_cols)\n",
    "\n",
    "# --- preprocessing with imputers ---\n",
    "numeric_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", numeric_pipe, [c for c in num_cols if c in Xtr.columns]),\n",
    "    (\"cat\", categorical_pipe, [c for c in cat_cols if c in Xtr.columns]),\n",
    "])\n",
    "\n",
    "def eval_model(pipe, name):\n",
    "    pipe.fit(Xtr, ytr)\n",
    "    for split, X, y in [(\"VAL\", Xva, yva), (\"TEST\", Xte, yte)]:\n",
    "        yhat = pipe.predict(X)\n",
    "        mae = mean_absolute_error(y, yhat)\n",
    "        rmse = mean_squared_error(y, yhat, squared=False)\n",
    "        r2 = r2_score(y, yhat)\n",
    "        print(f\"{name} [{split}]  MAE={mae:.3f}  RMSE={rmse:.3f}  R2={r2:.3f}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "lin = Pipeline([(\"prep\", pre), (\"reg\", LinearRegression())])\n",
    "eval_model(lin, \"Linear\")\n",
    "\n",
    "ridge = Pipeline([(\"prep\", pre), (\"reg\", Ridge(alpha=3.0, random_state=42))])\n",
    "eval_model(ridge, \"Ridge\")\n",
    "\n",
    "rf = Pipeline([(\"prep\", pre), (\"reg\", RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1))])\n",
    "eval_model(rf, \"RandomForest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67e8d8ff-fe07-408c-bb8c-1423eed29cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macsa\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macsa\\AppData\\Local\\Temp\\ipykernel_5532\\193476969.py:18: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  \"Spearman\": spearmanr(g_y, g_yhat, nan_policy=\"omit\").correlation\n",
      "C:\\Users\\macsa\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macsa\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macsa\\anaconda3\\Lib\\site-packages\\IPython\\core\\formatters.py:347: FutureWarning: Index.format is deprecated and will be removed in a future version. Convert using index.astype(str) or index.map(formatter) instead.\n",
      "  return method()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>position</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST</td>\n",
       "      <td>QB</td>\n",
       "      <td>1.323799</td>\n",
       "      <td>1.641513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST</td>\n",
       "      <td>WR</td>\n",
       "      <td>4.258850</td>\n",
       "      <td>5.755111</td>\n",
       "      <td>0.379485</td>\n",
       "      <td>0.669390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST</td>\n",
       "      <td>RB</td>\n",
       "      <td>4.307955</td>\n",
       "      <td>5.753037</td>\n",
       "      <td>0.425035</td>\n",
       "      <td>0.650492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  split position       MAE      RMSE        R2  Spearman\n",
       "0  TEST       QB  1.323799  1.641513  0.000000       NaN\n",
       "2  TEST       WR  4.258850  5.755111  0.379485  0.669390\n",
       "1  TEST       RB  4.307955  5.753037  0.425035  0.650492"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def eval_by_position(model, X, y, df_meta, pos_col=\"position\", label=\"TEST\"):\n",
    "    yhat = model.predict(X)\n",
    "    out = []\n",
    "    for p, g_idx in df_meta.groupby(pos_col).groups.items():\n",
    "        g_y = y[g_idx]; g_yhat = yhat[g_idx]\n",
    "        if len(g_y) < 20: \n",
    "            continue\n",
    "        out.append({\n",
    "            \"split\": label,\n",
    "            \"position\": p,\n",
    "            \"MAE\": mean_absolute_error(g_y, g_yhat),\n",
    "            \"RMSE\": mean_squared_error(g_y, g_yhat, squared=False),\n",
    "            \"R2\": r2_score(g_y, g_yhat),\n",
    "            \"Spearman\": spearmanr(g_y, g_yhat, nan_policy=\"omit\").correlation\n",
    "        })\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# assuming `ridge` is fitted from your last cell, and Xte/yte come from earlier\n",
    "bypos = eval_by_position(ridge, Xte, yte, test_df.reset_index(drop=True))\n",
    "display(bypos.sort_values(\"MAE\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09bd47e9-5585-4f5d-a54f-784655a3b0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge best: {'reg__alpha': 10.0}\n",
      "RF best: {'reg__max_depth': 14, 'reg__min_samples_leaf': 7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ridge_grid = GridSearchCV(\n",
    "    estimator=ridge, \n",
    "    param_grid={\"reg__alpha\":[0.1, 0.3, 1.0, 3.0, 10.0]},\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=3, n_jobs=-1\n",
    ").fit(Xtr, ytr)\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid={\"reg__max_depth\":[None, 8, 14], \"reg__min_samples_leaf\":[1, 3, 7]},\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=3, n_jobs=-1\n",
    ").fit(Xtr, ytr)\n",
    "\n",
    "print(\"Ridge best:\", ridge_grid.best_params_)\n",
    "print(\"RF best:\", rf_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cba28a1e-1e25-42b3-bad2-5ba68eb6b720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macsa\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoE [VAL]  MAE=4.006  RMSE=5.606  R2=0.409\n",
      "MoE [TEST]  MAE=4.099  RMSE=5.689  R2=0.412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macsa\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macsa\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "class MixedMoE(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, preprocessor, experts_by_pos, pos_col=\"position\"):\n",
    "        self.pre_template = preprocessor         # template to clone\n",
    "        self.experts_by_pos = experts_by_pos     # dict: pos -> estimator\n",
    "        self.pos_col = pos_col\n",
    "        self.gate = None\n",
    "        self.classes_ = None\n",
    "        self.experts_ = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Clone preprocessor for the gating model (don't reuse!)\n",
    "        gate_pre = clone(self.pre_template)\n",
    "        self.gate = Pipeline([\n",
    "            (\"prep\", gate_pre),\n",
    "            (\"clf\", LogisticRegression(max_iter=2000, multi_class=\"multinomial\"))\n",
    "        ])\n",
    "        self.gate.fit(X, X[self.pos_col])\n",
    "        self.classes_ = list(self.gate.named_steps[\"clf\"].classes_)\n",
    "\n",
    "        # Fit each expert on its slice, each with its own cloned preprocessor\n",
    "        self.experts_.clear()\n",
    "        for pos, base_model in self.experts_by_pos.items():\n",
    "            idx = (X[self.pos_col] == pos).values\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            model = Pipeline([\n",
    "                (\"prep\", clone(self.pre_template)),\n",
    "                (\"reg\", clone(base_model))\n",
    "            ])\n",
    "            model.fit(X.iloc[idx], y[idx])\n",
    "            self.experts_[pos] = model\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.gate.predict_proba(X)  # uses its own fitted pre\n",
    "        yhat = np.zeros(len(X))\n",
    "        for i, cls in enumerate(self.classes_):\n",
    "            if cls in self.experts_:\n",
    "                yhat += proba[:, i] * self.experts_[cls].predict(X)  # expert has its own fitted pre\n",
    "        return yhat\n",
    "\n",
    "# Use your tuned params\n",
    "ridge_alpha = ridge_grid.best_params_.get(\"reg__alpha\", 10.0) if 'ridge_grid' in globals() else 10.0\n",
    "rf_depth    = rf_grid.best_params_.get(\"reg__max_depth\", 14)    if 'rf_grid' in globals() else 14\n",
    "rf_leaf     = rf_grid.best_params_.get(\"reg__min_samples_leaf\", 7) if 'rf_grid' in globals() else 7\n",
    "\n",
    "experts = {\n",
    "    \"QB\": Ridge(alpha=ridge_alpha, random_state=42),\n",
    "    \"RB\": RandomForestRegressor(n_estimators=400, max_depth=rf_depth, min_samples_leaf=rf_leaf, random_state=42, n_jobs=-1),\n",
    "    \"WR\": RandomForestRegressor(n_estimators=400, max_depth=rf_depth, min_samples_leaf=rf_leaf, random_state=42, n_jobs=-1),\n",
    "    \"TE\": RandomForestRegressor(n_estimators=400, max_depth=rf_depth, min_samples_leaf=rf_leaf, random_state=42, n_jobs=-1),\n",
    "}\n",
    "\n",
    "moe = MixedMoE(preprocessor=pre, experts_by_pos=experts)\n",
    "moe.fit(Xtr, ytr)\n",
    "\n",
    "for tag, X, y in [(\"VAL\", Xva, yva), (\"TEST\", Xte, yte)]:\n",
    "    pred = moe.predict(X)\n",
    "    print(f\"MoE [{tag}]  MAE={mean_absolute_error(y,pred):.3f}  RMSE={mean_squared_error(y,pred, squared=False):.3f}  R2={r2_score(y,pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "668ffd80-b7f8-4cd6-a9fd-dca1dbd8ddda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macsa\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge [val]  MAE=4.184  RMSE=5.697  R2=0.390\n",
      "rf [val]  MAE=3.992  RMSE=5.582  R2=0.414\n",
      "moe [val]  MAE=4.006  RMSE=5.606  R2=0.409\n",
      "ridge [test]  MAE=4.263  RMSE=5.741  R2=0.401\n",
      "rf [test]  MAE=4.091  RMSE=5.665  R2=0.417\n",
      "moe [test]  MAE=4.099  RMSE=5.689  R2=0.412\n",
      "Saved outputs/feature_columns.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json, os\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# 1) Force-align columns for VAL/TEST to training feature set\n",
    "Xva = Xva.reindex(columns=Xtr.columns, fill_value=np.nan)\n",
    "Xte = Xte.reindex(columns=Xtr.columns, fill_value=np.nan)\n",
    "\n",
    "# 2) Rebuild best-tuned pipelines (fresh fit) so they exactly match current X*\n",
    "ridge_best = clone(ridge)   # ridge is your pipeline with pre+Ridge\n",
    "ridge_best.set_params(**{\"reg__alpha\": ridge_grid.best_params_.get(\"reg__alpha\", 10.0)})\n",
    "ridge_best.fit(Xtr, ytr)\n",
    "\n",
    "rf_best = clone(rf)         # rf is your pipeline with pre+RF\n",
    "rf_best.set_params(**{\n",
    "    \"reg__max_depth\": rf_grid.best_params_.get(\"reg__max_depth\", 14),\n",
    "    \"reg__min_samples_leaf\": rf_grid.best_params_.get(\"reg__min_samples_leaf\", 7),\n",
    "})\n",
    "rf_best.fit(Xtr, ytr)\n",
    "\n",
    "# (Optional) Refit MoE too, in case columns changed upstream\n",
    "moe.fit(Xtr, ytr)\n",
    "\n",
    "# 3) Utility: RMSE function without the deprecation warning\n",
    "try:\n",
    "    from sklearn.metrics import root_mean_squared_error as rmse_func\n",
    "except Exception:\n",
    "    rmse_func = lambda y_true, y_pred: mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "def evaluate_and_save(model, name, X, y, split):\n",
    "    yhat = model.predict(X)\n",
    "    metrics = {\n",
    "        \"MAE\": float(mean_absolute_error(y, yhat)),\n",
    "        \"RMSE\": float(rmse_func(y, yhat)),\n",
    "        \"R2\": float(r2_score(y, yhat))\n",
    "    }\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    with open(f\"outputs/metrics_{name}_{split}.json\",\"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"{name} [{split}]  MAE={metrics['MAE']:.3f}  RMSE={metrics['RMSE']:.3f}  R2={metrics['R2']:.3f}\")\n",
    "\n",
    "# 4) Save metrics for all three models on VAL and TEST\n",
    "for split, X, y in [(\"val\", Xva, yva), (\"test\", Xte, yte)]:\n",
    "    evaluate_and_save(ridge_best, \"ridge\", X, y, split)\n",
    "    evaluate_and_save(rf_best,    \"rf\",    X, y, split)\n",
    "    evaluate_and_save(moe,        \"moe\",   X, y, split)\n",
    "\n",
    "# 5) (Optional) Persist the exact feature set used for training (helps teammates reproduce)\n",
    "with open(\"outputs/feature_columns.json\",\"w\") as f:\n",
    "    json.dump(list(Xtr.columns), f, indent=2)\n",
    "print(\"Saved outputs/feature_columns.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a0d24-0d42-4f1f-b603-12a897a37830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
